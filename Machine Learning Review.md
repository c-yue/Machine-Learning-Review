


<br>



### General

#### Definition
- learn a target function f that maps input  
variables X to output variable Y, with an error e:  
$ğ‘Œ = f(ğ‘‹) + ğ‘’$

#### Parameters ~= Coeficient ~= Weight
- $\theta$
- they determine $f$

#### Goal
- find the best parameters making the $f$ works best  
<=> make the cost/loss small

#### Cost Function = Loss Function
- $J$, $J(\theta)$
- eg. $MSE=1/m \cdot \sum_{i=1}^m(\hat{y_i}-y_i)$  
<=> $MSE=1/m \cdot \sum_{i=1}^m(ğ‘“(x_i)-y_i)$  
$m$: the number of samples 

#### Bias-Variance trade-off
Bias: åè§ï¼Œé¢„æµ‹ç»“æœä¸å®é™…ç»“æœçš„ä¸åŒ  
Variance: æ–¹å·®ï¼Œé¢„æµ‹ç»“æœæœ¬èº«çš„æ³¢åŠ¨ï¼ˆå—è‡ªå˜é‡å½±å“ï¼‰



<br>



### Optimization

#### Gradient Descent - æ¢¯åº¦ä¸‹é™

- Aim: minimize the cost function, eg. MSE  

- Methodology:
    - æ¢¯åº¦çš„æ–¹å‘æ˜¯å‡½æ•°å¢é•¿é€Ÿåº¦æœ€å¿«çš„æ–¹å‘ï¼Œé‚£ä¹ˆ**æ¢¯åº¦çš„åæ–¹å‘å°±æ˜¯å‡½æ•°å‡å°‘æœ€å¿«çš„æ–¹å‘**ã€‚é‚£ä¹ˆï¼Œå¦‚æœæƒ³**è®¡ç®—ä¸€ä¸ªå‡½æ•°çš„æœ€å°å€¼**ï¼Œå°±å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ€æƒ³æ¥åšã€‚
    - å‡è®¾å¸Œæœ›æ±‚è§£ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼š $f({x})=f(x_{1},\cdots,x_{n})$  
        å¯ä»¥ä»ä¸€ä¸ªåˆå§‹ç‚¹ ${x}^{(0)}=(x_{1}^{(0)},\cdots,x_{n}^{(0)})$ å¼€å§‹ï¼ŒåŸºäºå­¦ä¹ ç‡ $\eta$ æ„å»ºä¸€ä¸ª**è¿­ä»£è¿‡ç¨‹**ï¼š  
        $x_{1}^{(i+1)} = x_{1}^{(i)} - \eta\cdot \frac{\partial f}{\partial x_{1}}({x}^{(i)})$,  
        $\cdots$  
        $x_{n}^{(i+1)} = x_{n}^{(i)} - \eta\cdot \frac{\partial f}{\partial x_{n}}({x}^{(i)})$  
    
    - å…¶ä¸­ ${x}^{(i)} = (x_{1}^{(i)},\cdots,x_{n}^{(i)})$ ï¼Œä¸€æ—¦è¾¾åˆ°æ”¶æ•›æ¡ä»¶ï¼Œè¿­ä»£å°±ç»“æŸã€‚
    ![plot](./images/gradient_decent.jpg)

- Batch Gradient Descent - æ‰¹é‡æ¢¯åº¦ä¸‹é™
    - use samples/batch for every iteration
- Stochastic Gradient Descent - SGD - éšæœºæ¢¯åº¦ä¸‹é™
    - use random samples/batch for every iteration
- **Algorithmn porcess of SGD**:  
    - Required: learning rate $\eta$, initialized parameters $\theta$
    - Repeat
        1. **select random m samples/batch from training set**:  
        samples with features ${x^{(1)},\cdots,x^{(m)}}$ and lables ${y^{(1)}, \cdots, y^{(m)}}$  
        2. **calculate gradient**:  
        $g = \nabla_{\theta} \sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})/m $  
        3. **parameters update**:  
        $\theta = \theta - \eta \cdot g$  
    - Until converge condition achieved
- ç‰›é¡¿æ³•æ¯”æ™®é€šæ¢¯åº¦ä¸‹é™æ›´å¿«çš„åŸå› ï¼Ÿ
    - æ¢¯åº¦ä¸‹é™çš„ä¸€é˜¶æ³°å‹’å±•å¼€å¼ï¼šhttps://blog.csdn.net/red_stone1/article/details/80212814
    - ç‰›é¡¿æ³•çš„äºŒé˜¶æ³°å‹’å±•å¼€å¼ï¼š


#### Ordinary Least Squares - æœ€å°äºŒä¹˜æ³•

- Linear Model: 
    - $h_{\theta}(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n$  
    - Matrix representation: $h_{\theta}(x) = X\theta$
- Goal: OLS is used to find the estimator/parameters $\theta$
- Method: 
    - minimizes the sum of squared residuals (Cost MSE)
    - æœ€å°äºŒä¹˜æ³•çš„ä»£æ•°æ³•è§£æ³•å°±æ˜¯**ç”¨æŸå¤±å‡½æ•°å¯¹ $\theta_i$ æ±‚åå¯¼æ•°ï¼Œä»¤åå¯¼æ•°ä¸º0ï¼Œå†è§£æ–¹ç¨‹ç»„**ï¼Œå¾—åˆ° $\theta_i$ ã€‚

- Steps:
    - **Cost MSE**: 
    $J(\theta) = 1/2 \cdot(X\theta-Y)^T(X\theta-Y)$
    - **ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°å¯¹å‘é‡ $\theta$ æ±‚å¯¼å–0**:
    $\frac{\partial }{\partial \theta}J(\theta)=X^T(X\theta-Y)=0$  
    çŸ©é˜µæ±‚å¯¼åŸç†ï¼Ÿ
    - **Result**:
    $\theta=(X^TX)^{-1}X^TY$

- Detailsï¼š
    - å…¶ä¸­ï¼Œå‡è®¾å‡½æ•° $h_{\theta}(x)$ ä¸º $m\cdot1$ çš„å‘é‡ï¼Œ $X$ ä¸º $m \cdot n$ çš„å‘é‡ï¼Œ $\theta$ ä¸º $n\cdot1$ çš„å‘é‡ï¼Œé‡Œé¢æœ‰ $n$ ä¸ªä»£æ•°æ³•çš„æ¨¡å‹å‚æ•°ã€‚ $m$ ä»£è¡¨æ ·æœ¬çš„ä¸ªæ•°ï¼Œ $n$ ä»£è¡¨æ ·æœ¬çš„ç‰¹å¾æ•°ã€‚


#### Maximum Likelihood Estimation - æå¤§ä¼¼ç„¶ä¼°è®¡æ³•

PS: review linear algebra
ä¸€é˜¶å¯¼
äºŒé˜¶å¯¼æ±‚æ³•
æ­£å®šçŸ©é˜µ
åŠæ­£å®šçŸ©é˜µ
hassionçŸ©é˜µ




<br>



### Linear Algorithms

Recognize linear/nonlinear:
- æ–¹æ³•ä¸€ï¼šåˆ¤åˆ«**å†³ç­–è¾¹ç•Œæ˜¯å¦æ˜¯ç›´çº¿**ã€‚çº¿æ¨¡å‹å¯ä»¥æ˜¯ç”¨æ›²çº¿æ‹Ÿåˆæ ·æœ¬ï¼Œä½†æ˜¯åˆ†ç±»çš„å†³ç­–è¾¹ç•Œä¸€å®šæ˜¯ç›´çº¿çš„ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ï¼›
- æ–¹æ³•äºŒï¼šåŒºåˆ†æ˜¯å¦ä¸ºçº¿æ€§æ¨¡å‹ï¼Œä¸»è¦æ˜¯çœ‹ä¸€ä¸ªä¹˜æ³•å¼å­ä¸­è‡ªå˜é‡ $x$ å‰çš„ç³»æ•° $w$ ï¼Œ**åº”è¯¥æ˜¯è¯´ $x_i$ åªè¢«ä¸€ä¸ª $w_i$ å½±å“ï¼Œé‚£ä¹ˆæ­¤æ¨¡å‹ä¸ºçº¿æ€§æ¨¡å‹**ï¼Œæˆ–è€…åˆ¤æ–­å†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„ï¼›
- ä¸¾ä¾‹ï¼š 
- $y=1/[1+exp(w_0+w_1x_1+w_2x_2)]$ ï¼Œç”»å‡º $y$ å’Œ $x$ æ˜¯æ›²çº¿å…³ç³»ï¼Œä½†æ˜¯å®ƒæ˜¯çº¿æ€§æ¨¡å‹ï¼Œå› ä¸º $w_1x_1$Â ä¸­å¯ä»¥è§‚å¯Ÿåˆ° $x_1$ åªè¢«ä¸€ä¸ª $w_1$ å½±å“ï¼›
- $y=1/[1+w_5 \cdot exp(w_0+w_1x_1+w_2x_2)]$ ï¼Œæ­¤æ¨¡å‹æ˜¯éçº¿æ€§æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ° $x_1$Â ä¸ä»…ä»…è¢«å‚æ•° $w_1$ å½±å“ï¼Œè¿˜è¢« $w_5$ å½±å“ï¼Œå¦‚æœè‡ªå˜é‡ $x$ è¢«ä¸¤ä¸ªä»¥ä¸Šçš„å‚æ•°å½±å“ï¼Œé‚£ä¹ˆæ­¤æ¨¡å‹æ˜¯éçº¿æ€§çš„ï¼›

#### Linear Regression

- Vectorization :
    - Representation
    ![plot](./images/vectorization.jpg)
    - Advantages
        - code shorter
        - algerbra libraries, GPU computing
        - calculate faster 
            - can be run seperately (parallel computing with parallel hardwares, boht in CPU and GPU) 
            - instead of running loop
        ![plot](./images/vectorization_efficient.jpg)
        ![plot](./images/vectorization_efficient2.jpg)



- Single 
- Multiple
- Polynomial - å¤šé¡¹å¼å›å½’

- Logistics - é€»è¾‘å›å½’

#### Logistic Regression
#### Linear Discriminant Analysis



<br>



### Nonlinear Algorithms

#### Classification and Regression Trees

- Decision Tree - å†³ç­–æ ‘ - for classification
    - Steps for building a decision tree:
        1. Start with all examples at the root node
        2. Calculate **information gain** for splitting **on all possible features**, and **pick the one** with the highest information gain
        3. **Split** dataset according to the selected feature, and create left and right branches of the tree
        4. Keep **repeating** splitting process until **stopping criteria** is met
    - Information Gain
        - the reduction in entropy that you get in your tree resulting from making a split
    - Entropy
        - shows the **randomness of the sample set**
        - Compute $p_1$, which is the fraction of examples that are edible (i.e. have value = `1` in `y`)
        - Entropy: $H(p_1) = -p_1 {log}_2(p_1) - (1- p_1) {log}_2(1- p_1)$
        - To expand: $H(p_1, p_2, ...p_n) = -p_1 {log}_2(p_1) -p_2 {log}_2(p_2) -...-p_n {log}_2(p_n)$
        - 0 perfect purity, 1 worst purity
    - Gini to replace Entropy
        - shows the **randomness of the sample set**
        - $Gini = \sum_{k=1}^n [p_k \cdot(1-p_k)]$
        - 0 perfect purity, 0.5 worst purity
    - Continuous Features
        - eg. 10 values in the feature, then try 9 split values to split tree
    ![plot](./images/tree_split.jpg)

- Decision Tree - å†³ç­–æ ‘ - for regression
    - Steps diff to classification tree:
        - **replace Entropy with Variance** to evaluate the Impure/Discrete
        - or can **replace Entropy with cost/MSE** 
    ![plot](./images/tree_split_regression.jpg)

- Advantages:
    - Easy to interpret and no overfitting with pruning
    - Works for both regression and classification problems
    - Can take any type of variables without modifications, and do not require any data preparation

- Disadvantages:
    - sensitive to sample changes

#### Naive Bayes Classifier
#### K-Nearest Neighbors
#### Support Vector Machines




<br>




### Ensemble Algorithms - é›†æˆç®—æ³•


#### Bagging and Random Forest
- Bagging
    - Deal with DT's sensitivity to sample changes
    - Bagging can reduce the variance of high-variance models
    - estimate a quantity from a sample by creating many random subsamples with replacement, and **computing the mean of each subsample model**.
- sampling with replacement - æœ‰æ”¾å›åœ°å–æ ·
    - eg. éœ€å–æ ·10ä¸ªï¼Œæ¯æ¬¡å–ä¸€ä¸ªåæ”¾å›å»ï¼Œå–åæ¬¡ï¼Œä½œä¸ºä¸€ä¸ªå­æ ·æœ¬é›†
    ![plot](./images/replace_sampling.jpg)
- Random Forest Steps
    1. given training set of size m
    2. for b = 1 to B:
        - sampling with replacement to create new training set with size m
        - given n features, choose $\sqrt{n}$ (for classification) or $n/3$ (for regression) to split decision tree
        - train decision tree on new data set
    3. bagged decision trees
- Feature selection
    - Bagged method can provide feature importance, by calculating and averaging the error function drop for individual variables
- Advantage:
    - Robust to overfitting and missing variables
    - Can be parallelized for distributed computing

#### Boosting and AdaBoost

#### GBDT and XGBoost











PCA & LDA
é™ç»´



### ç‰¹å¾å·¥ç¨‹
#### ç‰¹å¾æ„å»º
#### é¢„å¤„ç†




### Feature Selection - ç‰¹å¾é€‰æ‹©
https://zhuanlan.zhihu.com/p/507101225

#### Filter - è¿‡æ»¤æ³•
- Multicollinearity - åˆ é™¤å…·æœ‰å¤šé‡å…±çº¿æ€§çš„ç‰¹å¾ & Correlation - åˆ é™¤ä¸ç›¸å…³çš„ç‰¹å¾
    - æ•°å€¼å˜é‡ï¼š
        - Heatmap æŸ¥çœ‹å„ä¸ªç‰¹å¾ä¹‹é—´/ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
        - è®¾ç½®é˜ˆå€¼åˆ é™¤æŸäº›å…±çº¿ç‰¹å¾ï¼ˆeg. 0.8ï¼‰
        - è®¾ç½®é˜ˆå€¼åˆ é™¤ä¸ç›®æ ‡å˜é‡ä¸ç›¸å…³çš„ç‰¹å¾
    - ç±»åˆ«å˜é‡ï¼š
        - ç‰¹å¾ä¹‹é—´
            - å¡æ–¹æ£€éªŒï¼ˆæ£€éªŒä¸¤ä¸ªç‰¹å¾æ˜¯å¦ç›¸å…³ï¼‰
            - 0å‡è®¾ï¼šä¸¤ç‰¹å¾ä¸ç›¸å…³/ä¸¤ç‰¹å¾äº’ç›¸ç‹¬ç«‹
            - på°ï¼Œæ‹’ç»åŸå‡è®¾ï¼Œç‰¹å¾ç›¸å…³
            - ä¸¤ç‰¹å¾åˆ—è”è¡¨æ±‡æ€»ï¼ŒæŸ¥çœ‹å¡æ–¹å€¼å’Œpå€¼
            - p å€¼ <0.05ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ‹’ç»ç‰¹å¾ä¹‹é—´æ²¡æœ‰å…³è”çš„åŸå‡è®¾ï¼Œå³ä¸¤ä¸ªç‰¹å¾ä¹‹é—´å­˜åœ¨ç»Ÿè®¡ä¸Šæ˜¾ç€çš„å…³ç³»ã€‚
            - ç”±äºè¿™ä¸¤ä¸ªç‰¹å¾ä¹‹é—´å­˜åœ¨å…³è”ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©åˆ é™¤å…¶ä¸­ä¸€ä¸ªã€‚
        - ç›®æ ‡å˜é‡ä¸ç‰¹å¾ä¹‹é—´ï¼ŒåŒç†ï¼Œä½†åˆ é™¤ä¸ç›¸å…³çš„ç‰¹å¾
    - å¤šé‡å…±çº¿ç‹¬æœ‰ï¼š
        - VIF - æ–¹å·®è†¨èƒ€å› å­ï¼šæ•´ä½“æ¨¡å‹æ–¹å·®ä¸æ¯ä¸ªç‹¬ç«‹ç‰¹å¾çš„æ–¹å·®çš„æ¯”ç‡
            ```
            # calculate VIF 
            vif = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns) 
            ```
        - VIF = 1 è¡¨ç¤ºæ— ç›¸å…³æ€§ï¼ŒVIF = 1-5 ä¸­ç­‰ç›¸å…³æ€§ï¼ŒVIF >5 é«˜ç›¸å…³
        - åˆ é™¤é«˜VIFçš„ç‰¹å¾
- coefficient - ç‰¹å¾ç³»æ•°
    - ç³»æ•°æ˜¯å¦æ˜¾è‘—
        - è®¡ç®—å„ä¸ªç‰¹å¾å¯¹ç›®æ ‡å€¼çš„ç³»æ•°ä»¥åŠç³»æ•°çš„på€¼
        - 0å‡è®¾ï¼šè¯¥ç³»æ•°æ— å…³ï¼›æ­£å¼å‡è®¾ï¼šè¯¥ç³»æ•°æœ‰å…³
        - p < 0.05ï¼Œæ‹’ç»0å‡è®¾ï¼Œä½¿ç”¨è¯¥ç‰¹å¾
    - ç‰¹å¾è´¡çŒ®æ˜¯å¦è¶³å¤Ÿå¤§
        - å¦‚æœæ­£åœ¨è¿è¡Œå›å½’ä»»åŠ¡ï¼Œåˆ™ç‰¹å¾é€‚åº”åº¦çš„ä¸€ä¸ªå…³é”®æŒ‡æ ‡æ˜¯å›å½’ç³»æ•°ï¼ˆæ‰€è°“çš„ beta ç³»æ•°ï¼‰ï¼Œå®ƒæ˜¾ç¤ºäº†æ¨¡å‹ä¸­ç‰¹å¾çš„ç›¸å¯¹è´¡çŒ®ã€‚ æœ‰äº†è¿™äº›ä¿¡æ¯ï¼Œå¯ä»¥åˆ é™¤è´¡çŒ®å¾ˆå°æˆ–æ²¡æœ‰è´¡çŒ®çš„åŠŸèƒ½ã€‚






#### Wrapper - åŒ…è£…æ³•

#### Embedded - åµŒå…¥æ³•

#### Dimension Reduction - é™ç»´





coefficient ç³»æ•° - ä¾‹å¦‚çº¿æ€§æ¨¡å‹
Correlation ç›¸å…³æ€§