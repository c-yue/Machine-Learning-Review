


<br>



#### General

##### Definition
- learn a target function f that maps input  
variables X to output variable Y, with an error e:  
$ğ‘Œ = f(ğ‘‹) + ğ‘’$

##### Parameters ~= Coeficient ~= Weight
- $\theta$
- they determine $f$

##### Goal
- find the best parameters making the $f$ works best  
<=> make the cost/loss small

##### Cost Function = Loss Function
- $J$, $J(\theta)$
- eg. $MSE=1/m \cdot \sum_{i=1}^m(\hat{y_i}-y_i)$  
<=> $MSE=1/m \cdot \sum_{i=1}^m(ğ‘“(x_i)-y_i)$  
$m$: the number of samples 

##### Bias-Variance trade-off
Bias: åè§ï¼Œé¢„æµ‹ç»“æœä¸å®é™…ç»“æœçš„ä¸åŒ  
Variance: æ–¹å·®ï¼Œé¢„æµ‹ç»“æœæœ¬èº«çš„æ³¢åŠ¨ï¼ˆå—è‡ªå˜é‡å½±å“ï¼‰



<br>



#### Optimization

##### Gradient Descent - æ¢¯åº¦ä¸‹é™

- Aim: minimize the cost function, eg. MSE  

- Methodology:
    - æ¢¯åº¦çš„æ–¹å‘æ˜¯å‡½æ•°å¢é•¿é€Ÿåº¦æœ€å¿«çš„æ–¹å‘ï¼Œé‚£ä¹ˆ**æ¢¯åº¦çš„åæ–¹å‘å°±æ˜¯å‡½æ•°å‡å°‘æœ€å¿«çš„æ–¹å‘**ã€‚é‚£ä¹ˆï¼Œå¦‚æœæƒ³**è®¡ç®—ä¸€ä¸ªå‡½æ•°çš„æœ€å°å€¼**ï¼Œå°±å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ€æƒ³æ¥åšã€‚
    - å‡è®¾å¸Œæœ›æ±‚è§£ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼š $f({x})=f(x_{1},\cdots,x_{n})$  
        å¯ä»¥ä»ä¸€ä¸ªåˆå§‹ç‚¹ ${x}^{(0)}=(x_{1}^{(0)},\cdots,x_{n}^{(0)})$ å¼€å§‹ï¼ŒåŸºäºå­¦ä¹ ç‡ $\eta$ æ„å»ºä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼š  

        $x_{1}^{(i+1)} = x_{1}^{(i)} - \eta\cdot \frac{\partial f}{\partial x_{1}}({x}^{(i)})$,  
        $\cdots$  
        $x_{n}^{(i+1)} = x_{n}^{(i)} - \eta\cdot \frac{\partial f}{\partial x_{n}}({x}^{(i)})$  
        
    - å…¶ä¸­ ${x}^{(i)} = (x_{1}^{(i)},\cdots,x_{n}^{(i)})$ ï¼Œä¸€æ—¦è¾¾åˆ°æ”¶æ•›æ¡ä»¶ï¼Œè¿­ä»£å°±ç»“æŸã€‚

    ![plot](./images/gradient_decent.jpg)

- Batch Gradient Descent - æ‰¹é‡æ¢¯åº¦ä¸‹é™
    - use samples/batch for every iteration
- Stochastic Gradient Descent - SGD - éšæœºæ¢¯åº¦ä¸‹é™
    - use random samples/batch for every iteration
- **Algorithmn porcess of SGD**:  
    - Required: learning rate $\eta$, initialized parameters $\theta$
    - Repeat
        1. **select random m samples/batch from training set**:  
        samples with features ${x^{(1)},\cdots,x^{(m)}}$ and lables ${y^{(1)}, \cdots, y^{(m)}}$  
        2. **calculate gradient**:  
        $g = \nabla_{\theta} \sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})/m $  
        3. **parameters update**:  
        $\theta = \theta - \eta \cdot g$  
    - Until converge condition achieved


##### Ordinary Least Squares - æœ€å°äºŒä¹˜æ³•

- Linear Model: 
    - $h_{\theta}(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n$  
    - Matrix presentation: $h_{\theta}(x) = X\theta$
- Goal: OLS is used to find the estimator/parameters $\theta$
- Method: 
    - minimizes the sum of squared residuals (Cost MSE)
    - æœ€å°äºŒä¹˜æ³•çš„ä»£æ•°æ³•è§£æ³•å°±æ˜¯**ç”¨æŸå¤±å‡½æ•°å¯¹ $\theta_i$ æ±‚åå¯¼æ•°ï¼Œä»¤åå¯¼æ•°ä¸º0ï¼Œå†è§£æ–¹ç¨‹ç»„**ï¼Œå¾—åˆ° $\theta_i$ ã€‚

- Steps:
    - **Cost MSE**: 
    $J(\theta) = 1/2 \cdot(X\theta-Y)^T(X\theta-Y)$
    - **ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°å¯¹å‘é‡ $\theta$ æ±‚å¯¼å–0**:
    $\frac{\partial }{\partial \theta}J(\theta)=X^T(X\theta-Y)=0$  
    çŸ©é˜µæ±‚å¯¼åŸç†ï¼Ÿ
    - **Result**:
    $\theta=(X^TX)^{-1}X^TY$

- Detailsï¼š
    - å…¶ä¸­ï¼Œ å‡è®¾å‡½æ•° $h_{\theta}(x)$ ä¸º $m\cdot1$ çš„å‘é‡ï¼Œ $X$ ä¸º $m \cdot n$ çš„å‘é‡ï¼Œ $\theta$ ä¸º $n\cdot1$ çš„å‘é‡ï¼Œé‡Œé¢æœ‰ $n$ ä¸ªä»£æ•°æ³•çš„æ¨¡å‹å‚æ•°ã€‚ $m$ ä»£è¡¨æ ·æœ¬çš„ä¸ªæ•°ï¼Œ $n$ ä»£è¡¨æ ·æœ¬çš„ç‰¹å¾æ•°ã€‚









##### Maximum Likelihood Estimation

PS: review linear algebra
ä¸€é˜¶å¯¼
äºŒé˜¶å¯¼æ±‚æ³•
æ­£å®šçŸ©é˜µ
åŠæ­£å®šçŸ©é˜µ
hassionçŸ©é˜µ




<br>



#### Linear Algorithms

- specify linear/nonlinear
    - æ–¹æ³•ä¸€ï¼šåˆ¤åˆ«**å†³ç­–è¾¹ç•Œæ˜¯å¦æ˜¯ç›´çº¿**ã€‚çº¿æ¨¡å‹å¯ä»¥æ˜¯ç”¨æ›²çº¿æ‹Ÿåˆæ ·æœ¬ï¼Œä½†æ˜¯åˆ†ç±»çš„å†³ç­–è¾¹ç•Œä¸€å®šæ˜¯ç›´çº¿çš„ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ï¼›
    - æ–¹æ³•äºŒï¼šåŒºåˆ†æ˜¯å¦ä¸ºçº¿æ€§æ¨¡å‹ï¼Œä¸»è¦æ˜¯çœ‹ä¸€ä¸ªä¹˜æ³•å¼å­ä¸­è‡ªå˜é‡ $x$ å‰çš„ç³»æ•° $w$ ï¼Œ**åº”è¯¥æ˜¯è¯´ $x_i$ åªè¢«ä¸€ä¸ª $w_i$ å½±å“ï¼Œé‚£ä¹ˆæ­¤æ¨¡å‹ä¸ºçº¿æ€§æ¨¡å‹**ï¼Œæˆ–è€…åˆ¤æ–­å†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„ï¼›
    - ä¸¾ä¾‹ï¼š 
        -  $y=1/[1+exp(w_0+w_1x_1+w_2x_2)]$Â ï¼Œ ç”»å‡ºÂ $y$Â å’ŒÂ $x$Â æ˜¯æ›²çº¿å…³ç³»ï¼Œä½†æ˜¯å®ƒæ˜¯çº¿æ€§æ¨¡å‹ï¼Œå› ä¸ºÂ $w_1x_1$Â ä¸­å¯ä»¥è§‚å¯Ÿåˆ°Â $x_1$Â åªè¢«ä¸€ä¸ªÂ $w_1$Â å½±å“ï¼›
        -  $y=1/[1+w_5 \cdot exp(w_0+w_1x_1+w_2x_2)]$ ï¼Œ æ­¤æ¨¡å‹æ˜¯éçº¿æ€§æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ° $x_1$Â ä¸ä»…ä»…è¢«å‚æ•°Â $w_1$Â å½±å“ï¼Œè¿˜è¢«Â $w_5$Â å½±å“ï¼Œå¦‚æœè‡ªå˜é‡xè¢«ä¸¤ä¸ªä»¥ä¸Šçš„å‚æ•°å½±å“ï¼Œé‚£ä¹ˆæ­¤æ¨¡å‹æ˜¯éçº¿æ€§çš„ï¼›

##### Linear Regression
##### Logistic Regression
##### Linear Discriminant Analysis



<br>



#### Nonlinear Algorithms
##### Classification and Regression Trees
##### Naive Bayes Classifier
##### K-Nearest Neighbors
##### Support Vector Machines




<br>




#### Ensemble Algorithms
##### Bagging and Random Forest
##### Boosting and AdaBoost





















