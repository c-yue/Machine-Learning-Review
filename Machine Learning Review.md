

#### General

##### Definition
- learn a target function f that maps input  
variables X to output variable Y, with an error e:  
ğ‘Œ = ğ‘“(ğ‘‹) + ğ‘’

##### Parameters ~= Coeficient ~= Weight
- they determine ğ‘“

##### Aim
- find the best parameters making the f works best  
<=> make the cost/loss small

##### cost function = loss function
- eg. $MSE=1/m \cdot \sum_{i=1}^m(\hat{y_i}-y_i)$  
<=> $MSE=1/m \cdot \sum_{i=1}^m(ğ‘“(x_i)-y_i)$  
m: the number of samples 

##### Bias-Variance trade-off
Bias: åè§ï¼Œé¢„æµ‹ç»“æœä¸å®é™…ç»“æœçš„ä¸åŒ  
Variance: æ–¹å·®ï¼Œé¢„æµ‹ç»“æœæœ¬èº«çš„æ³¢åŠ¨ï¼ˆå—è‡ªå˜é‡å½±å“ï¼‰



<br>




#### Optimization

##### Gradient Descent - æ¢¯åº¦ä¸‹é™

![plot](./images/1666532209129.jpg)

- Batch Gradient Descend
- Stochastic Gradient Descent - SGD - éšæœºæ¢¯åº¦ä¸‹é™




##### Ordinary Least Squares

##### Maximum Likelihood Estimation


#### Linear Algorithms
##### Linear Regression
##### Logistic Regression
##### Linear Discriminant Analysis





#### Nonlinear Algorithms
##### Classification and Regression Trees
##### Naive Bayes Classifier
##### K-Nearest Neighbors
##### Support Vector Machines





#### Ensemble Algorithms
##### Bagging and Random Forest
##### Boosting and AdaBoost





















