


<br>



### General

#### Definition
- learn a target function f that maps input  
variables X to output variable Y, with an error e:  
$ğ‘Œ = f(ğ‘‹) + ğ‘’$

#### Parameters ~= Coeficient ~= Weight
- $\theta$
- they determine $f$

#### Goal
- find the best parameters making the $f$ works best  
<=> make the cost/loss small

#### Cost Function = Loss Function
- $J$, $J(\theta) = 1/2 \cdot \sum_{i=1}^m(\hat{y_i}-y_i)^2$
- eg. $MSE=1/m \cdot \sum_{i=1}^m(\hat{y_i}-y_i)^2$  
<=> $MSE=1/m \cdot \sum_{i=1}^m(ğ‘“(x_i)-y_i)^2$  
$m$: the number of samples 

#### Bias-Variance trade-off
Bias: åè§ï¼Œé¢„æµ‹ç»“æœä¸å®é™…ç»“æœçš„ä¸åŒ  
Variance: æ–¹å·®ï¼Œé¢„æµ‹ç»“æœæœ¬èº«çš„æ³¢åŠ¨ï¼ˆå—è‡ªå˜é‡å½±å“ï¼‰


----------------------------------------------------------------
<br>



### Optimization

#### Gradient Descent - æ¢¯åº¦ä¸‹é™

- Aim: minimize the cost function, eg. MSE  

- Methodology:
    - æ¢¯åº¦çš„æ–¹å‘æ˜¯å‡½æ•°å¢é•¿é€Ÿåº¦æœ€å¿«çš„æ–¹å‘ï¼Œé‚£ä¹ˆ**æ¢¯åº¦çš„åæ–¹å‘å°±æ˜¯å‡½æ•°å‡å°‘æœ€å¿«çš„æ–¹å‘**ã€‚é‚£ä¹ˆï¼Œå¦‚æœæƒ³**è®¡ç®—ä¸€ä¸ªå‡½æ•°çš„æœ€å°å€¼**ï¼Œå°±å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ€æƒ³æ¥åšã€‚
    - å‡è®¾å¸Œæœ›æ±‚è§£ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼š $f({x})=f(x_{1},\cdots,x_{n})$  
        å¯ä»¥ä»ä¸€ä¸ªåˆå§‹ç‚¹ ${x}^{(0)}=(x_{1}^{(0)},\cdots,x_{n}^{(0)})$ å¼€å§‹ï¼ŒåŸºäºå­¦ä¹ ç‡ $\eta$ æ„å»ºä¸€ä¸ª**è¿­ä»£è¿‡ç¨‹**ï¼š  
        $x_{1}^{(i+1)} = x_{1}^{(i)} - \eta\cdot \frac{\partial f}{\partial x_{1}}({x}^{(i)})$,  
        $\cdots$  
        $x_{n}^{(i+1)} = x_{n}^{(i)} - \eta\cdot \frac{\partial f}{\partial x_{n}}({x}^{(i)})$  
    
    - å…¶ä¸­ ${x}^{(i)} = (x_{1}^{(i)},\cdots,x_{n}^{(i)})$ ï¼Œä¸€æ—¦è¾¾åˆ°æ”¶æ•›æ¡ä»¶ï¼Œè¿­ä»£å°±ç»“æŸã€‚
    <img src="/images/gradient_decent.jpg" width="500" />  

- Batch Gradient Descent - æ‰¹é‡æ¢¯åº¦ä¸‹é™
    - use samples/batch for every iteration
- Stochastic Gradient Descent - SGD - éšæœºæ¢¯åº¦ä¸‹é™
    - use random samples/batch for every iteration
- **Algorithmn porcess of SGD**:  
    - Required: learning rate $\eta$, initialized parameters $\theta$
    - Repeat
        1. **select random m samples/batch from training set**:  
        samples with features ${x^{(1)},\cdots,x^{(m)}}$ and lables ${y^{(1)}, \cdots, y^{(m)}}$  
        2. **calculate gradient**:  
        $g = \nabla_{\theta} \sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)})/m $  
        3. **parameters update**:  
        $\theta = \theta - \eta \cdot g$  
    - Until converge condition achieved
- ç‰›é¡¿æ³•æ¯”æ™®é€šæ¢¯åº¦ä¸‹é™æ›´å¿«çš„åŸå› ï¼Ÿ
    - æ ¸å¿ƒæ€æƒ³ï¼Œç‰›é¡¿æ³•ç”¨äºŒæ¬¡å‡½æ•°æ‹Ÿåˆfï¼Œé¿å…æ¢¯åº¦ä¸‹é™æ—¶ä¸‹ä¸€ä¸ªç‚¹èµ°çš„å¤ªè¿œåè€Œå¢å¤§äº†fï¼Œåè€Œæ˜¯è®©ä¸‹ä¸€ä¸ªç‚¹èµ°åˆ°fâ€™â€™ä¸º0çš„åœ°æ–¹
        <img src="/images/ç‰›é¡¿æ³•äºŒæ¬¡å‡½æ•°æ‹Ÿåˆ.jpg" width="500" />  
    - https://zhuanlan.zhihu.com/p/59873169
    - https://zhuanlan.zhihu.com/p/37524275
    - æ¢¯åº¦ä¸‹é™çš„ä¸€é˜¶æ³°å‹’å±•å¼€å¼ï¼šhttps://blog.csdn.net/red_stone1/article/details/80212814
    - ç‰›é¡¿æ³•çš„äºŒé˜¶æ³°å‹’å±•å¼€å¼ï¼š


#### Ordinary Least Squares - æœ€å°äºŒä¹˜æ³•

- Linear Model: 
    - $h_{\theta}(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n$  
    - Matrix representation: $h_{\theta}(x) = X\theta$
- Goal: OLS is used to find the estimator/parameters $\theta$
- Method: 
    - minimizes the sum of squared residuals (Cost MSE)
    - æœ€å°äºŒä¹˜æ³•çš„ä»£æ•°æ³•è§£æ³•å°±æ˜¯**ç”¨æŸå¤±å‡½æ•°å¯¹ $\theta_i$ æ±‚åå¯¼æ•°ï¼Œä»¤åå¯¼æ•°ä¸º0ï¼Œå†è§£æ–¹ç¨‹ç»„**ï¼Œå¾—åˆ° $\theta_i$ ã€‚

- Steps:
    - **Cost MSE**: 
    $J(\theta) = 1/2 \cdot(X\theta-Y)^T(X\theta-Y)$
    - **ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°å¯¹å‘é‡ $\theta$ æ±‚å¯¼å–0**:
    $\frac{\partial }{\partial \theta}J(\theta)=X^T(X\theta-Y)=0$  
    çŸ©é˜µæ±‚å¯¼åŸç†ï¼Ÿ
    - **Result**:
    $\theta=(X^TX)^{-1}X^TY$

- Detailsï¼š
    - å…¶ä¸­ï¼Œå‡è®¾å‡½æ•° $h_{\theta}(x)$ ä¸º $m\cdot1$ çš„å‘é‡ï¼Œ $X$ ä¸º $m \cdot n$ çš„å‘é‡ï¼Œ $\theta$ ä¸º $n\cdot1$ çš„å‘é‡ï¼Œé‡Œé¢æœ‰ $n$ ä¸ªä»£æ•°æ³•çš„æ¨¡å‹å‚æ•°ã€‚ $m$ ä»£è¡¨æ ·æœ¬çš„ä¸ªæ•°ï¼Œ $n$ ä»£è¡¨æ ·æœ¬çš„ç‰¹å¾æ•°ã€‚


#### Maximum Likelihood Estimation - æå¤§ä¼¼ç„¶ä¼°è®¡æ³•

PS: review linear algebra
ä¸€é˜¶å¯¼
äºŒé˜¶å¯¼æ±‚æ³•
æ­£å®šçŸ©é˜µ
åŠæ­£å®šçŸ©é˜µ
hassionçŸ©é˜µ




----------------------------------------------------------------
<br>




### Model Selection & Model Evaluation

- **Model selection**: estimating the performance of different
models (hyperparameters) in order to choose the best  
- **Model evaluation** (assessment): having chosen a final model,
estimating its prediction error (generalization error) on new
data

- Validation Set
<img src="/images/split_data.jpg" width="500" />  

- Cross-validation
    - Cut the training set in k separate folds
    - For each fold, train on the (k-1) remaining folds
    - In practice: set k=5 or k=10
    <img src="/images/cross_validation_in_practice.jpg" width="500" />  


#### Classification Model Evaluation

- Confusion Matrix & Evaluation Metrics 
    <!-- <img src="/images/confusion_matrix.jpg" width="500" />   -->
    <img src="/images/confusion_matrix_and_evaluation_metrics.jpg" width="500" />  

    - False positives (false alarms) are also called type I errors
    False negatives (misses) are also called type II errors

    - åœ¨æ¨¡å‹é¢„æµ‹æ˜¯æ­£ä¾‹çš„æ‰€æœ‰ç»“æœä¸­ï¼Œæ¨¡å‹é¢„æµ‹å¯¹çš„æ¯”é‡
    Precision = Positive predictive value (PPV): $PPV=\frac{TP}{TP + FP}$  # of predicted positives
    
    - åœ¨çœŸå®å€¼æ˜¯æ­£ä¾‹çš„æ‰€æœ‰ç»“æœä¸­ï¼Œæ¨¡å‹é¢„æµ‹å¯¹çš„æ¯”é‡
    Recall = True positive rate (TPR): $TPR=\frac{TP}{TP + FN}$  # of positives

    <!-- - Specificity = True negative rate (TNR): $TNR=\frac{TN}{FP + TN}$
    False discovery rate (FDR): $FDR=\frac{FP}{FP + TP}$ -->

    - åˆ†ç±»æ¨¡å‹ä¸­æ‰€æœ‰åˆ¤æ–­æ­£ç¡®çš„ç»“æœå æ€»è§‚æµ‹å€¼å¾—æ¯”é‡
    Accuracy: $Acc=\frac{TP + TN}{TP + FN + FP + TN}$

    - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼
    F1-score: $F1=\frac{2TP}{2TP + FP + FN}$

- ROC Curve - Receiver-Operator Characteristic
    - AUC - Area Under Curve
    - ROC-AUC æŒ‡çš„æ˜¯ ROC æ›²çº¿ä¸‹çš„é¢ç§¯
    <img src="/images/ROC_AUC.jpg" width="500" /> 


- Hit Rate
    - å‘½ä¸­ç‡ï¼ˆHit Rateï¼ŒHRï¼‰ï¼Œå®ƒåæ˜ çš„æ˜¯åœ¨æ¨èåºåˆ—ä¸­æ˜¯å¦åŒ…å«äº†ç”¨æˆ·çœŸæ­£ç‚¹å‡»çš„itemï¼Œå…¬å¼å¦‚ä¸‹ï¼ŒNè¡¨ç¤ºæ¨èæ¬¡æ•°ï¼Œhit()å‡½æ•°è¡¨ç¤ºæ˜¯å¦å‘½ä¸­ï¼Œå³ç”¨æˆ·é€‰æ‹©çš„itemæ˜¯å¦åœ¨æ¨èåºåˆ—ä¸­ï¼Œå­˜åœ¨åˆ™ä¸º1ï¼Œåä¹‹åˆ™ä¸º0ã€‚
    $$HR = \frac{1}{N} \cdot \sum_{i=1}^N hit(i)$$

- Dice Coefficient
    - åˆ†å‰²è¿‡ç¨‹ä¸­çš„è¯„ä»·æ ‡å‡†ä¸»è¦é‡‡ç”¨Diceç›¸ä¼¼ç³»æ•°(Dice Similariy Coefficient,DSC),Diceç³»æ•°æ˜¯ä¸€ç§é›†åˆç›¸ä¼¼åº¦åº¦é‡æŒ‡æ ‡,é€šå¸¸ç”¨äºè®¡ç®—ä¸¤ä¸ªæ ·æœ¬çš„ç›¸ä¼¼åº¦,å€¼çš„èŒƒå›´  ,åˆ†å‰²ç»“æœæœ€å¥½æ—¶å€¼ä¸º1,æœ€å·®æ—¶å€¼ä¸º0
    $$Dice(P,T) = \frac{\left| P_{1} \wedge T_{1}  \right|}{(\left| P_{1} \right| + \left| T_{2} \right|)/2} \Leftrightarrow Dice = \frac{2TP}{FP+2TP+FN}$$
    <img src="/images/dice_coefficient.jpg" width="300" /> 






----------------------------------------------------------------
<br>





### Linear Algorithms

Recognize linear/nonlinear:
- æ–¹æ³•ä¸€ï¼šåˆ¤åˆ«**å†³ç­–è¾¹ç•Œæ˜¯å¦æ˜¯ç›´çº¿**ã€‚çº¿æ¨¡å‹å¯ä»¥æ˜¯ç”¨æ›²çº¿æ‹Ÿåˆæ ·æœ¬ï¼Œä½†æ˜¯åˆ†ç±»çš„å†³ç­–è¾¹ç•Œä¸€å®šæ˜¯ç›´çº¿çš„ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ï¼›
- æ–¹æ³•äºŒï¼šåŒºåˆ†æ˜¯å¦ä¸ºçº¿æ€§æ¨¡å‹ï¼Œä¸»è¦æ˜¯çœ‹ä¸€ä¸ªä¹˜æ³•å¼å­ä¸­è‡ªå˜é‡ $x$ å‰çš„ç³»æ•° $w$ ï¼Œ**åº”è¯¥æ˜¯è¯´ $x_i$ åªè¢«ä¸€ä¸ª $w_i$ å½±å“ï¼Œé‚£ä¹ˆæ­¤æ¨¡å‹ä¸ºçº¿æ€§æ¨¡å‹**ï¼Œæˆ–è€…åˆ¤æ–­å†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„ï¼›
- ä¸¾ä¾‹ï¼š 
- $y=1/[1+exp(w_0+w_1x_1+w_2x_2)]$ ï¼Œç”»å‡º $y$ å’Œ $x$ æ˜¯æ›²çº¿å…³ç³»ï¼Œä½†æ˜¯å®ƒæ˜¯çº¿æ€§æ¨¡å‹ï¼Œå› ä¸º $w_1x_1$Â ä¸­å¯ä»¥è§‚å¯Ÿåˆ° $x_1$ åªè¢«ä¸€ä¸ª $w_1$ å½±å“ï¼›
- $y=1/[1+w_5 \cdot exp(w_0+w_1x_1+w_2x_2)]$ ï¼Œæ­¤æ¨¡å‹æ˜¯éçº¿æ€§æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ° $x_1$Â ä¸ä»…ä»…è¢«å‚æ•° $w_1$ å½±å“ï¼Œè¿˜è¢« $w_5$ å½±å“ï¼Œå¦‚æœè‡ªå˜é‡ $x$ è¢«ä¸¤ä¸ªä»¥ä¸Šçš„å‚æ•°å½±å“ï¼Œé‚£ä¹ˆæ­¤æ¨¡å‹æ˜¯éçº¿æ€§çš„ï¼›

#### Linear Regression

- Vectorization :
    - Representation
        <img src="/images/vectorization.jpg" width="500" />    
        <img src="/images/matrix_multi.jpg" width="500" />    
    - Advantages
        - code shorter
        - algerbra libraries, GPU computing
        - calculate faster 
            - can be run seperately (parallel computing with parallel hardwares, boht in CPU and GPU) 
            - instead of running loop
        <img src="/images/vectorization_efficient.jpg" width="500" />  
        <img src="/images/vectorization_efficient2.jpg" width="500" />  

- Single - ä¸€å…ƒå›å½’
- Multiple - å¤šå…ƒå›å½’
- Polynomial - å¤šé¡¹å¼å›å½’
    - make new features by cross multiplying existing ones

- Lasso/Ridge Regression
    - cost func with regularization term, where $\lambda â‰¥ 0$ is a tuning parameter	to	be	determined

$$\begin{aligned}
J(\theta) &= 1/2 \cdot \sum_{i=1}^m(\hat{y_i}-y_i)^2\\
lasso\_reg &= \lambda \cdot \sum_{i=1}^m|\beta_i| \\ 
ridge\_reg &= \lambda \cdot \sum_{i=1}^m|\beta_i|^2\\
Cost &= J(\theta) + lasso\_reg | ridge\_reg
\end{aligned}$$


#### Logistic Regression

- for binary classification
- Logistic regression a linear method but predictions are transformed using the **logistic function (or sigmoid)**
<img src="/images/sigmoid.jpg" width="300" /> 


#### Linear Discriminant Analysis - çº¿æ€§åˆ¤åˆ«åˆ†æ
- åŸç†ï¼šç»™å®šè®­ç»ƒé›†æ ·ä¾‹ï¼Œè®¾æ³•å°†**æ ·ä¾‹æŠ•å½±åˆ°ä¸€æ¡ç›´çº¿**ä¸Šï¼Œä½¿å¾—**åŒç±»æ ·ä¾‹çš„æŠ•å½±ç‚¹å°½å¯èƒ½çš„æ¥è¿‘ã€å¼‚ç±»æ ·ä¾‹çš„æŠ•å½±ç‚¹å°½å¯èƒ½åœ°è¿œç¦»**ï¼›åœ¨å¯¹**æ–°æ ·æœ¬åˆ†ç±»æ—¶ï¼Œå°†å…¶æŠ•å½±ç‚¹åŒæ ·çš„æŠ•å½±åˆ°è¿™æ¡ç›´çº¿ä¸Š**ï¼Œå†æ ¹æ®æŠ•å½±ç‚¹çš„ä½ç½®æ¥ç¡®å®šæ–°æ ·ä¾‹çš„ä½ç½®
<img src="/images/LDA.jpg" width="300" />  

- LDA can be used for **dimensionality reduction** by keeping the latent variables as new variables

- LDA representation consists of statistical properties calculated for each class: means and the covariance matrix:  

$$\begin{aligned}
\mu_k &= \frac{1}{m_k} \sum_{i=1}^{m_k} x_i \\
S_k = \sigma_k^2 &= \frac{1}{m_k} \sum_{i=1}^{m_k} (x_i-\mu_k)^2
\end{aligned}$$

- Distribution diff inside classes in projected space:
    $$S_w = \sum_{k=1}^K S_k$$

- Distribution diff between classes (means diff, eg with 2 classes):
<div align=center><img src="/images/LDA_sb.jpg" width="700" /></div> 

- Algorithms Steps 
<img src="/images/LDA_steps.jpg" width="500" /> 

- ä¸PCAçš„åŒºåˆ«
    - PCAä¸»è¦æ˜¯ä»ç‰¹å¾çš„åæ–¹å·®è§’åº¦ï¼Œå»æ‰¾åˆ°æ¯”è¾ƒå¥½çš„æŠ•å½±æ–¹å¼ï¼Œå³é€‰æ‹©æ ·æœ¬ç‚¹æŠ•å½±å…·æœ‰æœ€å¤§æ–¹å·®çš„æ–¹å‘ï¼›
    - LDAåˆ™æ›´å¤šçš„æ˜¯è€ƒè™‘äº†åˆ†ç±»æ ‡ç­¾ä¿¡æ¯ï¼Œå¯»æ±‚æŠ•å½±åä¸åŒç±»åˆ«ä¹‹é—´æ•°æ®ç‚¹è·ç¦»æ›´å¤§åŒ–ä»¥åŠåŒä¸€ç±»åˆ«æ•°æ®ç‚¹è·ç¦»æœ€å°åŒ–ï¼Œå³**é€‰æ‹©åˆ†ç±»æ€§èƒ½æœ€å¥½çš„æ–¹å‘**ã€‚
    - PCAéœ€è¦ä¸å…¶ä»–ç®—æ³•ç»“åˆä½¿ç”¨
    - LDAæ˜¯ä¸€ç§ç›‘ç£å¼å­¦ä¹ æ–¹æ³•ï¼Œé™¤äº†å¯ä»¥é™ç»´å¤–ï¼Œè¿˜å¯ä»¥è¿›è¡Œé¢„æµ‹åº”ç”¨





----------------------------------------------------------------
<br>



### Nonlinear Algorithms

#### Classification and Regression Trees

- Decision Tree - å†³ç­–æ ‘ - for classification
    - Steps for building a decision tree:
        1. Start with all examples at the root node
        2. Calculate **information gain** for splitting **on all possible features**, and **pick the one** with the highest information gain
        3. **Split** dataset according to the selected feature, and create left and right branches of the tree
        4. Keep **repeating** splitting process until **stopping criteria** is met
    - Information Gain
        - the reduction in entropy that you get in your tree resulting from making a split
    - Entropy
        - shows the **randomness of the sample set**
        - Compute $p_1$, which is the fraction of examples that are edible (i.e. have value = `1` in `y`)
        - Entropy: $H(p_1) = -p_1 {log}_2(p_1) - (1- p_1) {log}_2(1- p_1)$
        - To expand: $H(p_1, p_2, ...p_n) = -p_1 {log}_2(p_1) -p_2 {log}_2(p_2) -...-p_n {log}_2(p_n)$
        - 0 perfect purity, 1 worst purity
    - Gini to replace Entropy
        - shows the **randomness of the sample set**
        - $Gini = \sum_{k=1}^n [p_k \cdot(1-p_k)]$
        - 0 perfect purity, 0.5 worst purity
    - Continuous Features
        - eg. 10 values in the feature, then try 9 split values to split tree
    <img src="/images/tree_split.jpg" width="500" />  

- Decision Tree - å†³ç­–æ ‘ - for regression
    - Steps diff to classification tree:
        - **replace Entropy with Variance** to evaluate the Impure/Discrete
        - or can **replace Entropy with cost/MSE** 
    <img src="/images/tree_split_regression.jpg" width="500" />  

- Advantages:
    - Easy to interpret and no overfitting with pruning
    - Works for both regression and classification problems
    - Can take any type of variables without modifications, and do not require any data preparation

- Disadvantages:
    - sensitive to sample changes

#### Naive Bayes Classifier
#### K-Nearest Neighbors
#### Support Vector Machines




----------------------------------------------------------------
<br>




### Ensemble Algorithms - é›†æˆå­¦ä¹ 

- Bagging: 
    - æ˜¯Bootstrap aggregatingçš„æ„æ€ï¼Œå„åˆ†ç±»å™¨ä¹‹é—´æ— å¼ºä¾èµ–ï¼Œå¯ä»¥å¹¶è¡Œã€‚
    - reduce variance æ–¹å·®
- Boosting: 
    - ä¸²è¡Œçš„æ–¹å¼è®­ç»ƒåŸºåˆ†ç±»å™¨ï¼Œå„åˆ†ç±»å™¨ä¹‹é—´æœ‰ä¾èµ–ã€‚
    - reduce bias åå·®
- Stacking
    - å…ˆç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒå¥½åŸºæ¨¡å‹ï¼Œç„¶åæ¯ä¸ªåŸºæ¨¡å‹éƒ½å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œçš„é¢„æµ‹
    - å…¶**é¢„æµ‹å€¼å°†ä½œä¸ºè®­ç»ƒæ ·æœ¬çš„ç‰¹å¾å€¼**ï¼Œæœ€ç»ˆä¼šå¾—åˆ°æ–°çš„è®­ç»ƒæ ·æœ¬
    - ç„¶ååŸºäºæ–°çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œè®­ç»ƒå¾—åˆ°æ¨¡å‹ï¼Œç„¶åå¾—åˆ°æœ€ç»ˆé¢„æµ‹ç»“æœ

#### Bagging & Random Forest - Bagging & éšæœºæ£®æ—
- Bagging
    - Deal with DT's sensitivity to sample changes
    - Bagging can reduce the variance of high-variance models
    - estimate a quantity from a sample by creating many random subsamples with replacement, and **computing the mean of each subsample model**.
- sampling with replacement - æœ‰æ”¾å›åœ°å–æ ·
    - eg. éœ€å–æ ·10ä¸ªï¼Œæ¯æ¬¡å–ä¸€ä¸ªåæ”¾å›å»ï¼Œå–åæ¬¡ï¼Œä½œä¸ºä¸€ä¸ªå­æ ·æœ¬é›†
    <img src="/images/replace_sampling.jpg" width="500" />  
- Random Forest Steps
    1. given training set of size m
    2. for b = 1 to B:
        - sampling with replacement to create new training set with size m
        - given n features, choose $\sqrt{n}$ (for classification) or $n/3$ (for regression) to split decision tree
        - train decision tree on new data set
    3. bagged decision trees
- Feature selection
    - Bagged method can provide feature importance, by calculating and averaging the error function drop for individual variables
- Advantage:
    - Robust to overfitting and missing variables
    - Can be parallelized for distributed computing

#### Boosting and AdaBoost

- AdaBoost - Adaptive Boosting
    - æ€æƒ³ï¼š
        - å¯¹åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬é™ä½æƒé‡
        - å¯¹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å‡é«˜æˆ–è€…ä¿æŒæƒé‡ä¸å˜
        - åœ¨æ¨¡å‹èåˆè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®é”™è¯¯ç‡å¯¹åŸºåˆ†ç±»å™¨å™¨è¿›è¡ŒåŠ æƒèåˆï¼Œé”™è¯¯ç‡ä½çš„åˆ†ç±»å™¨æ‹¥æœ‰æ›´å¤§çš„â€œè¯è¯­æƒâ€

#### GBDT and XGBoost
https://zhuanlan.zhihu.com/p/86263786

- GBDT - Gradient Boosting Decision Tree

    - åŸç† - GBDT ç”±ä¸‰ä¸ªæ¦‚å¿µç»„æˆ
        - **Regression Decision Tree** - DT
            - **æ¨¡å‹åŒ…å«å¤šæ£µæ ‘ï¼Œå°†æ‰€æœ‰é¢„æµ‹ç»“æœæƒ³åŠ ï¼Œå¾—åˆ°æœ€ç»ˆç»“æœ**
            - æ¨¡å‹çš„æ¯ä¸€è½®é¢„æµ‹éƒ½å’ŒçœŸå®å€¼æœ‰gapï¼Œè¿™ä¸ªgapç§°ä¸ºæ®‹å·®
            - **ä¸‹ä¸€è½®çš„æ ‘å¯¹æ®‹å·®è¿›è¡Œé¢„æµ‹**
            - $F_k(x) = \sum_{i=1}^{k}f_{i}(x)$
            - $F_k(x) = F_{k-1}(x)+f_{k}(x)$
        - **Gradient Boosting** - GB
            - æŸå¤±å‡½æ•°Loss: $J=\frac{1}{2}(y-F_{k}(x))^2$
            - **æ®‹å·®**å…¶å®æ˜¯**æœ€å°å‡æ–¹æŸå¤±å‡½æ•°Losså…³äºé¢„æµ‹å€¼çš„åå‘æ¢¯åº¦**ï¼š
            $-g = -\frac{\partial (\frac{1}{2}(y-F_{k}(x))^2)}{\partial F_k(x)} = y-F_{k}(x)$
            - é¢„æµ‹å€¼å’Œå®é™…å€¼çš„æ®‹å·®ä¸æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦ç›¸åŒ
            - **æ¯è®­ç»ƒä¸€æ£µæ ‘ï¼Œæ‹Ÿåˆæ®‹å·®/Lçš„è´Ÿæ¢¯åº¦ï¼Œè®©æ€»æ¨¡å‹åˆ©ç”¨è¿™æ£µæ ‘å¾€Lä¸‹é™çš„æ–¹å‘èµ°ï¼Œç±»ä¼¼äºæ¢¯åº¦ä¸‹é™**
        - **Shrinkage** å‰Šå¼±æ¯æ£µæ ‘çš„å½±å“
            - æ¯æ¬¡èµ°ä¸€å°æ­¥çš„æ–¹å¼é€æ¸é€¼è¿‘çœŸå®ç»“æœï¼Œè¿™æ ·æ¯”æ¯æ¬¡è¿ˆä¸€å¤§æ­¥çš„æ–¹å¼æ›´å®¹æ˜“é¿å…è¿‡æ‹Ÿåˆ
            - æ¯æ£µæ ‘åŠ å…¥åˆ°å‰ä¸€ä¸ªæ¨¡å‹å‰å¢åŠ ä¸€ä¸ªå­¦ä¹ ç‡/æ­¥é•¿ $\eta$
            - $F_i(x)=F_{i-1}(x)+\mu f_i(x)$
    - GBDT çš„æ¯ä¸€æ­¥æ®‹å·®è®¡ç®—å…¶å®å˜ç›¸åœ°**å¢å¤§äº†è¢«åˆ†é”™æ ·æœ¬çš„æƒé‡**ï¼Œè€Œå¯¹äº**åˆ†å¯¹æ ·æœ¬çš„æƒé‡è¶‹äº0**ï¼Œè¿™æ ·åé¢çš„æ ‘å°±èƒ½ä¸“æ³¨äºé‚£äº›è¢«åˆ†é”™çš„æ ·æœ¬
    - Gradientè¢«ç”¨æ¥è®©Losså¿«é€Ÿä¸‹é™ï¼Œè¿›è€Œè®©æ¨¡å‹æ•ˆæœBoost
    - GBDTä½¿ç”¨çš„å¼±å­¦ä¹ å™¨å¿…é¡»æ˜¯å›å½’æ ‘ã€‚GBDTç”¨æ¥åšå›å½’é¢„æµ‹ï¼Œå½“ç„¶ï¼Œé€šè¿‡è®¾ç½®é˜ˆå€¼ä¹Ÿèƒ½ç”¨äºåˆ†ç±»ä»»åŠ¡
    - **Steps**:
        <img src="/images/GBDT_steps.jpg" width="500" />  

- XGBoost - Extreme Gradient Boosting Decision Tree
    - XGBoost æ˜¯å¤§è§„æ¨¡å¹¶è¡Œ boosting tree çš„å·¥å…·
    - Diff to GBDT
    https://zhuanlan.zhihu.com/p/42740654
        - ä¼ ç»ŸGBDTä»¥CARTä½œä¸ºåŸºåˆ†ç±»å™¨ï¼ŒXGBoostè¿˜**æ”¯æŒçº¿æ€§åˆ†ç±»å™¨**ï¼Œè¿™ä¸ªæ—¶å€™XGBoostç›¸å½“äºå¸¦L1å’ŒL2æ­£åˆ™åŒ–é¡¹çš„é€»è¾‘æ–¯è’‚å›å½’ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰æˆ–è€…çº¿æ€§å›å½’ï¼ˆå›å½’é—®é¢˜ï¼‰ã€‚
        - ä¼ ç»ŸGBDTåœ¨ä¼˜åŒ–æ—¶åªç”¨åˆ°ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼ŒXGBooståˆ™å¯¹ä»£ä»·å‡½æ•°è¿›è¡Œäº†**äºŒé˜¶æ³°å‹’å±•å¼€ï¼ŒåŒæ—¶ç”¨åˆ°äº†ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°**ã€‚
        - XGBooståœ¨**ä»£ä»·å‡½æ•°é‡ŒåŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦**ã€‚æ­£åˆ™é¡¹é‡ŒåŒ…å«äº†æ ‘çš„å¶å­èŠ‚ç‚¹ä¸ªæ•°ã€æ¯ä¸ªå¶å­èŠ‚ç‚¹ä¸Šè¾“å‡ºçš„scoreçš„L2æ¨¡çš„å¹³æ–¹å’Œã€‚ä»Bias-variance tradeoffè§’åº¦æ¥è®²ï¼Œæ­£åˆ™é¡¹é™ä½äº†æ¨¡å‹çš„varianceï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè¿™ä¹Ÿæ˜¯XGBoostä¼˜äºä¼ ç»ŸGBDTçš„ä¸€ä¸ªç‰¹æ€§ã€‚
        - **Shrinkage**ï¼ˆç¼©å‡ï¼‰ï¼Œç›¸å½“äºå­¦ä¹ é€Ÿç‡ï¼ˆXGBoostä¸­çš„etaï¼‰ã€‚XGBooståœ¨è¿›è¡Œå®Œä¸€æ¬¡è¿­ä»£åï¼Œä¼šå°†å¶å­èŠ‚ç‚¹çš„æƒé‡ä¹˜ä¸Šè¯¥ç³»æ•°ï¼Œä¸»è¦æ˜¯ä¸ºäº†å‰Šå¼±æ¯æ£µæ ‘çš„å½±å“ï¼Œè®©åé¢æœ‰æ›´å¤§çš„å­¦ä¹ ç©ºé—´ã€‚å®é™…åº”ç”¨ä¸­ï¼Œä¸€èˆ¬æŠŠetaè®¾ç½®å¾—å°ä¸€ç‚¹ï¼Œç„¶åè¿­ä»£æ¬¡æ•°è®¾ç½®å¾—å¤§ä¸€ç‚¹ã€‚ï¼ˆè¡¥å……ï¼šä¼ ç»ŸGBDTçš„å®ç°ä¹Ÿæœ‰å­¦ä¹ é€Ÿç‡ï¼‰
        - **åˆ—æŠ½æ ·ï¼ˆcolumn subsamplingï¼‰**ã€‚XGBoostå€Ÿé‰´äº†**éšæœºæ£®æ—**çš„åšæ³•ï¼Œæ”¯æŒåˆ—æŠ½æ ·ï¼Œä¸ä»…èƒ½é™ä½è¿‡æ‹Ÿåˆï¼Œè¿˜èƒ½å‡å°‘è®¡ç®—ï¼Œè¿™ä¹Ÿæ˜¯XGBoostå¼‚äºä¼ ç»Ÿgbdtçš„ä¸€ä¸ªç‰¹æ€§ã€‚
        - å¯¹**ç¼ºå¤±å€¼**çš„å¤„ç†ã€‚å¯¹äºç‰¹å¾çš„å€¼æœ‰ç¼ºå¤±çš„æ ·æœ¬ï¼ŒXGBoostå¯ä»¥è‡ªåŠ¨å­¦ä¹ å‡ºå®ƒçš„åˆ†è£‚æ–¹å‘ã€‚
        - XGBoostå·¥å…·æ”¯æŒ**å¹¶è¡Œ**ã€‚boostingä¸æ˜¯ä¸€ç§ä¸²è¡Œçš„ç»“æ„å—?æ€ä¹ˆå¹¶è¡Œçš„ï¼Ÿæ³¨æ„XGBoostçš„å¹¶è¡Œ**ä¸æ˜¯treeç²’åº¦çš„å¹¶è¡Œï¼ŒXGBoostä¹Ÿæ˜¯ä¸€æ¬¡è¿­ä»£å®Œæ‰èƒ½è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£çš„**ã€‚XGBoostçš„å¹¶è¡Œæ˜¯åœ¨**ç‰¹å¾ç²’åº¦ä¸Š**çš„ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œå†³ç­–æ ‘çš„å­¦ä¹ æœ€è€—æ—¶çš„ä¸€ä¸ªæ­¥éª¤å°±æ˜¯å¯¹ç‰¹å¾çš„å€¼è¿›è¡Œæ’åºï¼ˆå› ä¸ºè¦ç¡®å®šæœ€ä½³åˆ†å‰²ç‚¹ï¼‰ï¼Œ**XGBooståœ¨è®­ç»ƒä¹‹å‰ï¼Œé¢„å…ˆå¯¹æ•°æ®è¿›è¡Œäº†æ’åºï¼Œç„¶åä¿å­˜ä¸ºblockç»“æ„ï¼Œåé¢çš„è¿­ä»£ä¸­é‡å¤åœ°ä½¿ç”¨è¿™ä¸ªç»“æ„**ï¼Œå¤§å¤§å‡å°è®¡ç®—é‡ã€‚**è¿™ä¸ªblockç»“æ„ä¹Ÿä½¿å¾—å¹¶è¡Œæˆä¸ºäº†å¯èƒ½**ï¼Œåœ¨è¿›è¡ŒèŠ‚ç‚¹çš„åˆ†è£‚æ—¶ï¼Œéœ€è¦è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¢ç›Šï¼Œæœ€ç»ˆé€‰å¢ç›Šæœ€å¤§çš„é‚£ä¸ªç‰¹å¾å»åšåˆ†è£‚ï¼Œé‚£ä¹ˆå„ä¸ªç‰¹å¾çš„å¢ç›Šè®¡ç®—å°±å¯ä»¥å¼€å¤šçº¿ç¨‹è¿›è¡Œã€‚




----------------------------------------------------------------
<br>




### Nueral Network - ç¥ç»ç½‘ç»œ

#### Basic Nueral Network

- Notation 
    - layer 0 input with n nodes
    - layer 1 with $w^{[1]}_{1,2,3,...}$ -> n1 nodes
    - layer 2 with $w^{[2]}_{1,2,3,...}$ -> n2 nodes
    <img src="/images/NN_compute.jpg" width="500" />   

- Activation Funcs
    - why use activation?
        - manay layers come to be one layer -> NN same as linear model
    - why sigmoid not enough?
        - It assumes that awareness is maybe binary - either people are aware or they are not
        - Maybe awareness should be any non negative number
        - ReLU instead
        <img src="/images/sigmoid_not_enough.jpg" width="500" />  
    - Choosing which to use
        - target/ouput layer
            - sigmoid good for binary
            - original linear for regression
            - ReLU for none neg prediction
        - hidden layer - ReLU common
            - compute faster
            - learning faster because no flat result in the sigmoid (close to -1/1 when activated values are extreme)

- Multi class & Softmax

- softmax

$$a_j = \frac{e^{z_j}}{ \sum_{k=0}^{N-1}{e^{z_k} }}$$

$$\begin{aligned}
    \mathbf{a}(x) =
    \begin{bmatrix}
        P(y = 1 | \mathbf{x}; \mathbf{w},b) \\
        \vdots \\
        P(y = N | \mathbf{x}; \mathbf{w},b)
        \end{bmatrix}
        =
        \frac{1}{ \sum_{k=1}^{N}{e^{z_k} }}
        \begin{bmatrix}
        e^{z_1} \\
        \vdots \\
        e^{z_{N}} \\
    \end{bmatrix} 
\end{aligned}$$

<div align=center><img src="./images/softmax.jpg" width="500"/></div>
    
- softmax cost 

$$\begin{aligned} 
    L(\mathbf{a},y)=\begin{cases} 
        -log(a_1), & \text{if y=1}.\\
            &\vdots\\
        -log(a_N), & \text{if y=N} 
    \end{cases} 
\end{aligned}$$
        
<div align=center><img src="./images/softmax_cost.jpg" width="500" /></div>  

- Adam Gradient Decent
    - if **a parameter w_j, or b seems to keep on moving in roughly the same direction**. graph_left -> **increase the learning rate for that parameter** & go faster in that direction. 
    - Conversely, if **a parameter keeps oscillating back and forth**. graph_right -> **reduce Alpha_j for that parameter** &  not have it keep on oscillating or bouncing back and forth.  
    <img src="./images/adam.jpg" width="500" />  

- Back Propagation
    - Notation  
    <img src="./images/back_prop.jpg" width="500" />  
    <img src="./images/back_prop_in_nn.jpg" width="500" />  

    - advantage: æ¯å±‚åå‘ä¼ æ’­éƒ½ä¼šå­˜å‚¨ç”¨äºä¸‹ä¸€å±‚ï¼Œæ•…å°±ç®—æ‰€æœ‰è¾“å…¥å‚æ•°çš„æ¢¯åº¦æ—¶ï¼Œä¸­é—´æ¢¯åº¦ä¸ç”¨é‡å¤è®¡ç®—ï¼ˆåŠ¨æ€è§„åˆ’ï¼‰
    <img src="./images/back_prop_efficient.jpg" width="300" />  




#### Converlutional Nueral Network - CNN - å·ç§¯ç¥ç»ç½‘ç»œ
<img src="/images/CNN_signal_app.jpg" width="500" />   



----------------------------------------------------------------
<br>




### ç‰¹å¾å·¥ç¨‹
#### ç‰¹å¾æ„å»º
#### é¢„å¤„ç†




#### Feature Selection - ç‰¹å¾é€‰æ‹©
https://zhuanlan.zhihu.com/p/507101225

##### Filter - è¿‡æ»¤æ³•
- Multicollinearity - åˆ é™¤å…·æœ‰å¤šé‡å…±çº¿æ€§çš„ç‰¹å¾ & Correlation - åˆ é™¤ä¸ç›¸å…³çš„ç‰¹å¾
    - æ•°å€¼å˜é‡ï¼š
        - Heatmap æŸ¥çœ‹å„ä¸ªç‰¹å¾ä¹‹é—´/ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
        - è®¾ç½®é˜ˆå€¼åˆ é™¤æŸäº›å…±çº¿ç‰¹å¾ï¼ˆeg. 0.8ï¼‰
        - è®¾ç½®é˜ˆå€¼åˆ é™¤ä¸ç›®æ ‡å˜é‡ä¸ç›¸å…³çš„ç‰¹å¾
    - ç±»åˆ«å˜é‡ï¼š
        - ç‰¹å¾ä¹‹é—´
            - å¡æ–¹æ£€éªŒï¼ˆæ£€éªŒä¸¤ä¸ªç‰¹å¾æ˜¯å¦ç›¸å…³ï¼‰
            - 0å‡è®¾ï¼šä¸¤ç‰¹å¾ä¸ç›¸å…³/ä¸¤ç‰¹å¾äº’ç›¸ç‹¬ç«‹
            - på°ï¼Œæ‹’ç»åŸå‡è®¾ï¼Œç‰¹å¾ç›¸å…³
            - ä¸¤ç‰¹å¾åˆ—è”è¡¨æ±‡æ€»ï¼ŒæŸ¥çœ‹å¡æ–¹å€¼å’Œpå€¼
            - p å€¼ <0.05ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ‹’ç»ç‰¹å¾ä¹‹é—´æ²¡æœ‰å…³è”çš„åŸå‡è®¾ï¼Œå³ä¸¤ä¸ªç‰¹å¾ä¹‹é—´å­˜åœ¨ç»Ÿè®¡ä¸Šæ˜¾ç€çš„å…³ç³»ã€‚
            - ç”±äºè¿™ä¸¤ä¸ªç‰¹å¾ä¹‹é—´å­˜åœ¨å…³è”ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©åˆ é™¤å…¶ä¸­ä¸€ä¸ªã€‚
        - ç›®æ ‡å˜é‡ä¸ç‰¹å¾ä¹‹é—´ï¼ŒåŒç†ï¼Œä½†åˆ é™¤ä¸ç›¸å…³çš„ç‰¹å¾
    - å¤šé‡å…±çº¿ç‹¬æœ‰ï¼š
        - VIF - æ–¹å·®è†¨èƒ€å› å­ï¼šæ•´ä½“æ¨¡å‹æ–¹å·®ä¸æ¯ä¸ªç‹¬ç«‹ç‰¹å¾çš„æ–¹å·®çš„æ¯”ç‡
            ```
            # calculate VIF 
            vif = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns) 
            ```
        - VIF = 1 è¡¨ç¤ºæ— ç›¸å…³æ€§ï¼ŒVIF = 1-5 ä¸­ç­‰ç›¸å…³æ€§ï¼ŒVIF >5 é«˜ç›¸å…³
        - åˆ é™¤é«˜VIFçš„ç‰¹å¾
- coefficient - ç‰¹å¾ç³»æ•°
    - ç³»æ•°æ˜¯å¦æ˜¾è‘—
        - è®¡ç®—å„ä¸ªç‰¹å¾å¯¹ç›®æ ‡å€¼çš„ç³»æ•°ä»¥åŠç³»æ•°çš„på€¼
        - 0å‡è®¾ï¼šè¯¥ç³»æ•°æ— å…³ï¼›æ­£å¼å‡è®¾ï¼šè¯¥ç³»æ•°æœ‰å…³
        - p < 0.05ï¼Œæ‹’ç»0å‡è®¾ï¼Œä½¿ç”¨è¯¥ç‰¹å¾
    - ç‰¹å¾è´¡çŒ®æ˜¯å¦è¶³å¤Ÿå¤§
        - å¦‚æœæ­£åœ¨è¿è¡Œå›å½’ä»»åŠ¡ï¼Œåˆ™ç‰¹å¾é€‚åº”åº¦çš„ä¸€ä¸ªå…³é”®æŒ‡æ ‡æ˜¯å›å½’ç³»æ•°ï¼ˆæ‰€è°“çš„ beta ç³»æ•°ï¼‰ï¼Œå®ƒæ˜¾ç¤ºäº†æ¨¡å‹ä¸­ç‰¹å¾çš„ç›¸å¯¹è´¡çŒ®ã€‚ æœ‰äº†è¿™äº›ä¿¡æ¯ï¼Œå¯ä»¥åˆ é™¤è´¡çŒ®å¾ˆå°æˆ–æ²¡æœ‰è´¡çŒ®çš„åŠŸèƒ½ã€‚

##### Wrapper - åŒ…è£…æ³•
- å‰å‘é€‰æ‹©
    - ä» 0 ç‰¹å¾å¼€å§‹ï¼Œç„¶åæ·»åŠ ä¸€ä¸ªæœ€å¤§ç¨‹åº¦åœ°å‡å°‘é”™è¯¯çš„ç‰¹å¾ï¼›ç„¶åæ·»åŠ å¦ä¸€ä¸ªç‰¹å¾ï¼Œä¾æ­¤ç±»æ¨ã€‚
- å‘åé€‰æ‹©
    - æ¨¡å‹ä»åŒ…å«çš„æ‰€æœ‰ç‰¹å¾å¼€å§‹å¹¶è®¡ç®—è¯¯å·®ï¼›ç„¶åå®ƒæ¶ˆé™¤äº†ä¸€ä¸ªå¯ä»¥è¿›ä¸€æ­¥å‡å°‘è¯¯å·®çš„ç‰¹å¾ã€‚ é‡å¤è¯¥è¿‡ç¨‹ï¼Œç›´åˆ°ä¿ç•™æ‰€éœ€æ•°é‡çš„ç‰¹å¾ã€‚
##### Embedded - åµŒå…¥æ³•
- Random Forest / GBDT / XGBoost çš„ feature_importance

##### Dimension Reduction - é™ç»´
PCA & LDA
ä»åº”ç”¨çš„è§’åº¦ï¼Œ**å¯¹æ— ç›‘ç£çš„ä»»åŠ¡ä½¿ç”¨PCA è¿›è¡Œé™ç»´ï¼Œå¯¹æœ‰ç›‘ç£çš„åˆ™åº”ç”¨LDA**


----------------------------------------------------------------
<br>



### Recommender System - æ¨èç³»ç»Ÿ

#### Methodology
- Retrieval - Candidate Generate
    - eg. collaborative Filtering, top 10 items in user's 3 hot catogaries
    - delete duplicates from different retrieval items
- Ranking
    - eg. FM/Content based method

#### Collaborative Filtering - ååŒè¿‡æ»¤

#### Content Based Filtering - åŸºäºå†…å®¹çš„è¿‡æ»¤

- Generates a user and item feature vector 
- with user features & item features in Nueral Network
    - The user features & item features is provided to a neural network which then generates the user and movie vector as shown below.
    - two networks that are combined by a dot product
    <img src="./images/content_based_filtering_nn.jpg" width="500" />  
- Minimize the following cost
    $$J = \sum_{i,j:r(i,j)=1}(v_u^{j} \cdot v_m^{i} - y^{(i,j)})^2 +\text{regularization}$$
- Find similar items:
    - A similarity measure is the squared distance between the two vectors $v_m^{(k)}$ and $v_m^{(i)}$: $$\left\Vert v_m^{(k)} - v_m^{(i)} \right\Vert^2 = \sum_{l=1}^{n}(v_{m_l}^{(k)} - v_{m_l}^{(i)})^2\tag{1}$$






----------------------------------------------------------------
<br>

coefficient ç³»æ•° - ä¾‹å¦‚çº¿æ€§æ¨¡å‹
Correlation ç›¸å…³æ€§